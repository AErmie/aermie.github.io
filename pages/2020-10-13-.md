---
layout: page
title: Book Review: Kubernetes in Action, Second Edition
date: 2020-10-13 11:42
author: AdinErmie
comments: true
categories: []
---
<a href="/wp-content/uploads/2020/07/KubernetesInAction-SecondEdition-BookCover.png"><img class="alignleft size-medium wp-image-33410" src="/wp-content/uploads/2020/07/KubernetesInAction-SecondEdition-BookCover-240x300.png" alt="" width="240" height="300" /></a>Recently, I finished reading <a href="https://www.manning.com/books/kubernetes-in-action-second-edition" target="_blank" rel="noopener noreferrer">Kubernetes in Action, Second Edition</a> by Marko Lukša.

I've recently been ramping up on Kubernetes, so, I had some previous knowledge and hands-on experience with Kubernetes when I read this book.

I particularly found<span style="color: #000000;"> the whole book helpful but in particular Chapter 5 ("<strong>Handling common failures in AKS</strong>"), Chapter 7 ("<strong>Monitoring the AKS cluster and the application</strong>"), and Chapter 10 ("<strong>Securing your AKS cluster</strong>").</span>

<span style="color: #ff0000;">Even though I don't have a lot of highlights to share, I really liked this book. I found it most useful because it provides progressive hands-on experience with Kubernetes. My challenge, being an IT Pro, has always been the fact that I don't have the skills to create an app, just to use it as an example/reference for something else I was doing.</span>

<span style="color: #ff0000;">This book provides several example applications (in container form, obviously) so that you have pods/services/etc. deployed to work through the concepts in the book.</span>

<span style="color: #ff0000;">I’ve decided to share my highlights from reading this specific publication, in case the points that I found of note/interest will be of some benefit to someone else. So, here are my highlights (by chapter). Note that not every chapter will have highlights (depending on the content and the main focus of my work).</span>
<h2>Chapter 1: Introduction Kubernetes</h2>
<ul>
 	<li>The word Kubernetes is Greek for pilot or helmsman, the person who steers the ship - the person standing at the helm (the ship’s wheel).</li>
 	<li>Kubernetes is a software system for automating the deployment and management of complex, large-scale application systems composed of computer processes running in containers.</li>
 	<li>The engineers responsible for operating the system can focus on the big picture instead of wasting time on the details.</li>
 	<li>Kubernetes was originally developed by Google. Google has practically always run applications in containers. As early as 2014, it was reported that they start two billion containers every week. That’s over 3,000 containers per second, and the figure is much higher today.</li>
 	<li><strong>Note:</strong> Data on Google’s energy use suggests that they run around 900,000 servers.</li>
 	<li>When the system consists of many microservices, automated management is crucial. Kubernetes provides this automation. The features it offers make the task of managing hundreds of microservices almost trivial.</li>
 	<li>Any company that wants to be able to move its applications from one provider to another will have to make additional, initially unnecessary efforts to abstract the infrastructure and APIs of the underlying cloud provider from the applications. This requires resources that could otherwise be focused on building the primary business logic.</li>
 	<li>By relying on Kubernetes to provide these features, application developers can focus on implementing the core business logic instead of wasting time integrating applications with the infrastructure.</li>
 	<li>The master nodes will run the Kubernetes Control Plane, which represents the brain of your system and controls the cluster, while the rest will run your applications - your workloads - and will therefore represent the Workload Plane.</li>
 	<li>Non-production clusters can use a single master node, but highly available clusters use at least three physical master nodes to host the Control Plane. The number of worker nodes depends on the number of applications you’ll deploy.</li>
 	<li>Kubernetes may later even move the application from one node to another. You may not even notice when that happens, and you shouldn’t care.</li>
 	<li>When a node fails, Kubernetes automatically moves applications to the remaining healthy nodes. The operations team no longer needs to manually move the application and can instead focus on repairing the node itself and returning it to the pool of available hardware resources.</li>
 	<li>Everything in Kubernetes is represented by an object. You create and retrieve these objects via the Kubernetes API.</li>
 	<li>If you are considering introducing Kubernetes in your organization, the most important question you need to answer is whether you’ll manage Kubernetes yourself or use a Kubernetes-as-a-Service type offering where someone else manages it for you.</li>
 	<li>Kubernetes brings with it an enormous amount of additional complexity. Anyone who wants to run a Kubernetes cluster must be intimately familiar with its inner workings.</li>
 	<li>Using Kubernetes is ten times easier than managing it.</li>
 	<li>The open-source version of Kubernetes is maintained by the community and represents the cutting edge of Kubernetes development. This also means that it may not be as stable as the other options. It may also lack good security defaults. Deploying the vanilla version requires a lot of fine tuning to set everything up for production use.</li>
 	<li>The first thing you need to be honest about is whether you need to automate the management of your applications at all. If your application is a large monolith, you definitely don’t need Kubernetes.</li>
 	<li>But if your system consists of less than five microservices, throwing Kubernetes into the mix is probably not a good idea. If your system has more than twenty microservices, you will most likely benefit from the integration of Kubernetes.</li>
</ul>
<h2>Chapter 2: Understanding containers</h2>
<ul>
 	<li>A container is nothing more than an isolated process running in the existing host OS that consumes only the resources the app consumes. They have virtually no overhead.</li>
 	<li>In fact, you should never run multiple applications in the same container, as this makes managing the processes in the container much more difficult.</li>
 	<li>All existing software dealing with containers, including Kubernetes itself, is designed under the premise that there’s only one application in a container.</li>
 	<li>Running the application on any computer is made possible by the fact that the environment of the application is decoupled from the environment of the host.</li>
 	<li>The Linux distribution installed on the host is irrelevant. The only thing that might be important is the kernel version and the kernel modules it loads.</li>
 	<li>The filesystems are isolated by the Copy-on-Write (CoW) mechanism. The filesystem of a container consists of read-only layers from the container image and an additional read/write layer stacked on top. When an application running in container A changes a file in one of the read-only layers, the entire file is copied into the container’s read/write layer and the file contents are changed there. Since each container has its own writable layer, changes to shared files are not visible in any other container.</li>
 	<li>If a containerized application requires a particular kernel version, it may not work on every computer. If a computer is running a different version of the Linux kernel or doesn’t load the required kernel modules, the app can’t run on it.</li>
 	<li>It should also be clear that a containerized app built for a specific hardware architecture can only run on computers with the same architecture. You can’t put an application compiled for the x86 CPU architecture into a container and expect to run it on an ARM-based computer just because Docker is available there.</li>
 	<li>If you refer to images without explicitly specifying the tag, Docker assumes that you’re referring to the special latest tag.</li>
 	<li>The -t option specifies the desired image name and tag and the dot at the end specifies the path to the directory that contains the Dockerfile and the build context (all artefacts needed by the build process).</li>
 	<li><strong>Tip:</strong> Don’t add unnecessary files to the build directory, as they will slow down the build process—especially if the Docker daemon is located on a remote system.</li>
 	<li>When building an image, a new layer is created for each individual directive in the Dockerfile.</li>
 	<li>To learn about RUN and other directives you can use in a Dockerfile, refer to the Dockerfile reference at https://docs.docker.com/engine/reference/builder/.</li>
 	<li>Deleting a file with a subsequent directive won’t reduce the size of the image. If you use the RUN directive, make sure that the command it executes deletes all temporary files it creates before it terminates.</li>
 	<li>To remove all dangling images, you can also use the docker image prune command.</li>
 	<li>A process can only see and use the resources in its own namespaces. It can’t use any in other namespaces.</li>
 	<li><strong>Note:</strong> The shell’s executable file must be available in the container’s file system. This isn’t always the case with containers running in production.</li>
 	<li>The -it option is shorthand for two options:
<ul>
 	<li>-i tells Docker to run the command in interactive mode. · -t tells it to allocate a pseudo terminal (TTY) so you can use the shell properly. You need both if you want to use the shell the way you’re used to. If you omit the first, you can’t execute any commands, and if you omit the second, the command prompt doesn’t appear and some commands may complain that the TERM variable is not set.</li>
 	<li>Linux Control Groups (cgroups ). It limits, accounts for and isolates system resources such as CPU, memory and disk or network bandwidth. When using cgroups, a process or group of processes can only use the allotted CPU time, memory, and network bandwidth for example. This way, processes cannot occupy resources that are reserved for other processes.</li>
</ul>
</li>
 	<li><strong>Note:</strong> With Docker you create a privileged container by using the --privileged flag.</li>
 	<li>Docker and Kubernetes drop all capabilities except those required by typical applications, but users can add or drop other capabilities if authorized to do so.</li>
 	<li>If you need even finer control over what sys-calls a program can make, you can use seccomp (Secure Computing Mode). You can create a custom seccomp profile by creating a JSON file that lists the sys-calls that the container using the profile is allowed to make. You then provide the file to Docker when you create the container.</li>
 	<li>With SELinux, you attach labels to files and system resources, as well as to users and processes. A user or process can only access a file or resource if the labels of all subjects and objects involved match a set of policies. AppArmor is similar but uses file paths instead of labels and focuses on processes rather than users.</li>
</ul>
<h2>Chapter 3: Deploying your first application</h2>
<ul>
 	<li>Setting up a full-fledged, multi-node Kubernetes cluster isn’t a simple task, especially if you’re not familiar with Linux and network administration.</li>
 	<li>If you’re new to Kubernetes, the safest bet is to start with Minikube</li>
 	<li>Instead of using Docker to run containers, nodes created by kind use the CRI-O container runtime</li>
 	<li>Proper management of Kubernetes clusters is incredibly difficult. The installation alone is a task not to be underestimated.</li>
 	<li>Once you’ve successfully deployed one or two clusters using kubeadm, you can then try to deploy it completely manually, by following Kelsey Hightower’s Kubernetes the Hard Way tutorial at github.com/kelseyhightower/Kubernetes-the-hard-way. Though you may run into several problems, figuring out how to solve them can be a great learning experience.</li>
 	<li>To use kubectl, you must both install it and prepare the kubeconfig file so kubectl knows what cluster to talk to.</li>
 	<li><strong>Tip:</strong> You can always append --help to any kubectl command to get information on what it does and how to use it.</li>
 	<li>On Windows, if you use the Command Prompt, define the alias by executing doskey k=kubectl $*. If you use PowerShell, execute set-alias -name k -value kubectl.</li>
 	<li>To see more detailed information about an object, you use the kubectl describe command</li>
 	<li><strong>Tip:</strong> Executing the describe command without specifying the object name is useful when only one object of a certain type exists. You don’t have to type or copy/paste the object name.</li>
 	<li>In Kubernetes, instead of deploying individual containers, you deploy groups of co-located containers – so-called pods.</li>
 	<li>You can think of each pod as a separate logical computer that contains one application. The application can consist of a single process running in a container, or a main application process and additional supporting processes, each running in a separate container.</li>
 	<li>You can display a list of all supported types by running kubectl api-resources. The list also shows the short name for each type and some other information you need to define objects in JSON/YAML files</li>
 	<li>Instead of telling Kubernetes what to do, you simply set a new desired state of the system and let Kubernetes achieve it. To do this, it examines the current state, compares it with the desired state, identifies the differences and determines what it must do to reconcile them.</li>
 	<li><strong>Note:</strong> You can also use the -o wide output option to see additional information when listing other object types.</li>
 	<li>The Deployment object represents an application deployment. It specifies which container image contains your application and how many replicas of the application Kubernetes should run. Each replica is represented by a Pod object. The Service object represents a single communication entry point to these replicas.</li>
 	<li>If the service is backed by multiple pods, it acts as a load balancer. But even there is only one pod, you still want to expose it through a service.</li>
 	<li>Unlike pods, services aren’t ephemeral. When you create a service, it is assigned a static IP address that never changes during lifetime of the service.</li>
</ul>
<h2>Chapter 4: Introducing the Kubernetes API objects</h2>
<ul>
 	<li>A resource isn’t the same as an object. If you are familiar with relational database systems, you can compare resources and object types with views and tables. Resources are views through which you interact with objects.</li>
 	<li>The manifest of most Kubernetes API objects consists of the following four sections:
<ul>
 	<li><strong>Type</strong> Metadata contains information about the type of object this manifest describes. It specifies the object type, the group to which the type belongs, and the API version.</li>
 	<li><strong>Object</strong> Metadata holds the basic information about the object instance, including its name, time of creation, owner of the object, and other identifying information. The fields in the Object Metadata are the same for all object types.</li>
 	<li><strong>Spec</strong> is the part in which you specify the desired state of the object. Its fields differ between different object types. For pods, this is the part that specifies the pod’s containers, storage volumes and other information related to its operation.</li>
 	<li><strong>Status</strong> contains the current actual state of the object. For a pod, it tells you the condition of the pod, the status of each of its containers, its IP address, the node it’s running on, and other information that reveals what’s happening to your pod.</li>
</ul>
</li>
 	<li>All Kubernetes API objects contain the two metadata sections, but not all have the Spec and Status sections.</li>
 	<li><strong>Note:</strong> Use the -o json option to display the object in JSON instead of YAML.</li>
 	<li>The API server uses TLS and you typically need a client certificate or token for authentication.</li>
 	<li>To learn more about individual fields in the manifest, you can refer to the API reference documentation at http://kubernetes.io/docs/reference/ or use the kubectl explain command</li>
 	<li><strong>Note:</strong> If you want to see the complete structure of an object (the complete hierarchical list of fields without the descriptions), try kubectl explain pods --recursive.</li>
 	<li><strong>Tip:</strong> The jq tool is very handy if you want to see only a part of the object’s structure. For example, to display the node’s status conditions, you can run kubectl get node -o json | jq .status.conditions. The equivalent tool for YAML is yq.</li>
 	<li>A more user-friendly way to inspect an object is the kubectl describe command, which typically displays the same information or sometimes even more.</li>
 	<li>Unlike other objects, each Event object is deleted one hour after its creation to reduce the burden on etcd, the data store for Kubernetes API objects.</li>
 	<li>Because Events are standalone objects, you can list them using kubectl get events</li>
 	<li>As with other objects, the kubectl get command only outputs the most important object data. To display additional information, you can enable additional columns by executing the command with the -o wide option</li>
 	<li>YAML is supposed to be easy for people to read, but the alphabetical field order in Kubernetes YAML breaks this.</li>
</ul>
<h2>Chapter 5: Running applications in Pods</h2>
<ul>
 	<li>Although pods may contain several, it’s not uncommon for a pod to contain just a single container.</li>
 	<li>Containers are designed to run only a single process, not counting any child processes that it spawns. Both container tooling and Kubernetes were developed around this fact.</li>
 	<li>Another indication that containers should only run a single process is the fact that the container runtime only restarts the container when the container’s root process dies. It doesn’t care about any child processes created by this root process. If it spawns child processes, it alone is responsible for keeping all these processes running.</li>
 	<li><strong>Note:</strong> When containers of the same pod use separate PID namespaces, they can’t see each other or send process signals like SIGTERM or SIGINT between them.</li>
 	<li>It’s this sharing of certain namespaces that gives the processes running in a pod the impression that they run together, even though they run in separate containers.</li>
 	<li>If a container has to be scaled separately from the other components, this is a clear indication that it must be deployed in a separate pod.</li>
 	<li>Placing several containers in a single pod is only appropriate if the application consists of a primary process and one or more processes that complement the operation of the primary process.</li>
 	<li>Adding a sidecar increases the pod’s resources requirements because an additional process must run in the pod.</li>
 	<li>When deciding whether to use the sidecar pattern and place containers in a single pod, or to place them in separate pods, ask yourself the following questions:
<ul>
 	<li>Do these containers have to run on the same host?</li>
 	<li>Do I want to manage them as a single unit?</li>
 	<li>Do they form a unified whole instead of being independent components?</li>
 	<li>Do they have to be scaled together?</li>
 	<li>Can a single node meet their combined resource needs?</li>
</ul>
</li>
 	<li>As a rule of thumb, always place containers in separate pods unless a specific reason requires them to be part of the same pod.</li>
 	<li><strong>Note:</strong> The decision whether to use YAML or JSON to define your objects is yours. Most people prefer to use YAML because it’s slightly more human-friendly and allows you to add comments to the object definition.</li>
 	<li><strong>Tip:</strong> Whenever you want to create a pod manifest from scratch, you can also use the following command to create the file and then edit it to add more fields:
<ul>
 	<li>kubectl run kubia --image=luksa/kubia:1.0 --dry-run=client -o yaml &gt; mypod.yaml</li>
 	<li>The --dry-run=client flag tells kubectl to output the definition instead of actually creating the object via the API.</li>
</ul>
</li>
 	<li>It’s a good idea to always specify the ports so that anyone who has access to your cluster can see which ports each pod exposes. By explicitly defining ports, you can also assign a name to each port, which is very useful when you expose pods via services.</li>
 	<li><strong>Note:</strong> The kubectl port-forward command can also forward connections to services instead of pods and has several other useful features. Run kubectl port-forward --help to learn more.</li>
 	<li><strong>Tip:</strong> You can display timestamps by only typing --timestamps without the value. For boolean options, merely specifying the option name sets the option to true. This applies to all kubectl options that take a Boolean value and default to false.</li>
 	<li><strong>Note:</strong> Kubectl options that take a value can be specified with an equal sign or with a space. Instead of --tail=10, you can also type --tail 10.</li>
 	<li>Kubernetes keeps a separate log file for each container. They are usually stored in /var/log/containers on the node that runs the container. A separate file is created for each container. If the container is restarted, its logs are written to a new file.</li>
 	<li>Kubectl offers the cp command to copy files or directories from your local computer to a container of any pod or from the container to your computer.</li>
 	<li>The double dash (--) in the command delimits kubectl arguments from the command to be executed in the container.</li>
 	<li>To keep images small and improve security in the container, most containers used in production don’t contain any binary files other than those required for the container’s primary process. This significantly reduces the attack surface, but also means that you can’t run shells or other tools in production containers.</li>
 	<li>The kubectl attach command is another way to interact with a running container. It attaches itself to the standard input, output and error streams of the main process running in the container.</li>
 	<li>If the application running in a pod wants to read from standard input, you must indicate this in the pod manifest by setting the stdin field in the container definition to true.</li>
 	<li><strong>Note:</strong> Like the kubectl exec command, kubectl attach also supports the --tty or -t option, which indicates that the standard input is a terminal (TTY), but the container must be configured to allocate a terminal through the tty field in the container definition.</li>
 	<li><strong>Note:</strong> An additional field in the container definition, stdinOnce, determines whether the standard input channel is closed when the attach session ends. It’s set to false by default, which allows you to use the standard input in every kubectl attach session. If you set it to true, standard input remains open only during the first session.</li>
 	<li><strong>Note:</strong> If you don’t provide the name, kubectl exec defaults to the first container specified in the pod manifest.</li>
 	<li>When a pod contains more than one container, all the containers are started in parallel. Kubernetes doesn’t yet provide a mechanism to specify whether a container depends on another container, which would allow you to ensure that one is started before the other.</li>
 	<li>Init containers are like the pod’s regular containers, but they don’t run in parallel - only one init container runs at a time.</li>
 	<li>By moving tools or data that could be used by an attacker to compromise your cluster from the main container to an init container, you reduce the pod’s attack surface.</li>
 	<li>In a pod manifest, init containers are defined in the initContainers field in the spec section, just as regular containers are defined in its containers field.</li>
 	<li><strong>Tip:</strong> By default, the kubectl delete command waits until the object no longer exists. To skip the wait, run the command with the --wait=false option.</li>
 	<li>When you delete a pod associated with the Deployment, the controller immediately creates a replacement pod.</li>
</ul>
<h2>Chapter 6: Managing the lifecycle of the Pod’s containers</h2>
<ul>
 	<li>The kubectl describe command only shows whether each condition is true or not. To find out why a condition is false, you must inspect the pod manifest</li>
 	<li>To see the status of individual containers, you must use kubectl describe</li>
 	<li><strong>Tip:</strong> You can also display the container status(es) using jq like this: kubectl get po kubia -o json | jq .status.containerStatuses</li>
 	<li>When a pod is scheduled to a node, the Kubelet on that node starts its containers and from then on keeps them running for as long as the pod object exists. If the main process in the container terminates for any reason, the Kubelet restarts the container. If an error in your application causes it to crash, Kubernetes automatically restarts it</li>
 	<li><strong>Note:</strong> Surprisingly, the restart policy is configured at the pod level and applies to all its containers. It can’t be configured for each container individually.</li>
 	<li>The first time a container terminates, it is restarted immediately. The next time, however, Kubernetes waits ten seconds before restarting it again. This delay is then doubled to 20, 40, 80 and then to 160 seconds after each subsequent termination. From then on, the delay is kept at five minutes. This delay that doubles between attempts is called exponential back-off.</li>
 	<li><strong>Note:</strong> The delay is reset to zero when the container has run successfully for 10 minutes.</li>
 	<li>If the application doesn’t respond within one second, the probe attempt is considered failed. If it fails three times in a row, the container is considered unhealthy and is terminated.</li>
 	<li>The parameter initialDelaySeconds determines how long Kubernetes should delay the execution of the first probe after starting the container. The periodSeconds field specifies the amount of time between the execution of two consecutive probes, whereas the timeoutSeconds field specifies how long to wait for a response before the probe attempt counts as failed. The failureThreshold field specifies how many times the probe must fail for the container to be considered unhealthy and potentially restarted.</li>
 	<li>For applications that don’t expose HTTP health-check endpoints, the tcpSocket or the exec liveness probes should be used.</li>
 	<li>The default liveness probe settings give the application between 20 and 30 seconds to start responding to liveness probe requests. If the application takes longer to start, it is restarted and must start again. If the second start also takes as long, it is restarted again. If this continues, the container never reaches the state where the liveness probe succeeds and gets stuck in an endless restart loop.</li>
 	<li>If a startup probe is defined for a container, only the startup probe is executed when the container is started. The startup probe can be configured to take into account the slow start of the application. When the startup probe succeeds, Kubernetes switches to using the liveness probe, which is configured to quickly detect when the application becomes unhealthy.</li>
 	<li>Usually, the startup and liveness probes are configured to use the same HTTP endpoint, but different endpoints can be used. You can also configure the startup probe as an exec or tcpSocket probe instead of an httpGet probe.</li>
 	<li>The CPU and memory consumed by the probe handler invocation count towards the resource quota of the container, so using a resource-intensive handler will reduce the CPU time available to the main process of the application.</li>
 	<li>You may also want to run additional processes every time a container starts and just before it stops. You can do this by adding lifecycle hooks to the container.</li>
 	<li><strong>Note:</strong> The same as with liveness probes, lifecycle hooks can only be applied to regular containers and not to init containers. Unlike probes, lifecycle hooks do not support tcpSocket handlers.</li>
 	<li><strong>Tip:</strong> Because the state of a pod can change quickly, inspecting just its status may not tell you everything you need to know. Rather than inspecting the state at a particular moment in time, reviewing the pod’s events is usually a better way to get the full picture.</li>
 	<li><strong>Note:</strong> You can’t specify both an exec and an httpGet post-start hook for a container. You can only specify one.</li>
 	<li>You shouldn’t use a pre-stop hook to perform an action that needs to be performed when the entire pod is shut down, because pre-stop hooks run every time the container needs to terminate.</li>
 	<li>Warning If the imagePullPolicy is set to Always and the image registry is offline, the container will not run even if the same image is already stored locally.</li>
 	<li>The Kubelet doesn’t start all containers of the pod at the same time. It creates and starts the containers synchronously in the order they are defined in the pod’s spec.</li>
 	<li>Perhaps surprisingly, if the restart policy is set to Never and the startup hook fails, the pod’s status is shown as Completed even though the post-start hook failed.</li>
 	<li>The termination of each container at pod shutdown follows the same sequence as when the container is terminated because it has failed its liveness probe, except that instead of the termination grace period, the pod’s deletion grace period determines how much time is available to the containers to shut down on their own.</li>
 	<li><strong>Tip:</strong> You should make sure that your init containers also handle the TERM signal so that they shut down immediately if you delete the pod object while it’s still being initialized.</li>
</ul>
<h2>Chapter 7: Mounting storage volumes into the Pod’s containers</h2>
<ul>
 	<li>When a container starts, the files in its filesystem are those that were added to its container image during build time.</li>
 	<li>The lifecycle of a volume is tied to the lifecycle of the entire pod and is independent of the lifecycle of the container in which it is mounted. Due to this fact, volumes are also used to persist data across container restarts.</li>
 	<li><strong>Tip:</strong> Before you mount a volume in a container to preserve files across container restarts, consider how this affects the container’s self-healing capability.</li>
 	<li>In general, each volume definition must include a name and a type, which is indicated by the name of the nested field</li>
 	<li>An emptyDir volume is also useful for sharing files between containers of the same pod (it can’t be used to share files between containers of different pods).</li>
 	<li><strong>Tip:</strong> You can see what node a pod is scheduled to by running kubectl get po -o wide.</li>
 	<li>Typically, the underlying storage technology doesn’t allow a volume to be attached to more than one node at a time in read/write mode, but multiple pods on the same node can all use the volume in read/write mode.</li>
 	<li>A hostPath volume points to a specific file or directory in the filesystem of the host node</li>
 	<li>Typically, a hostPath volume is used in cases where the pod actually needs to read or write files written or read by the node itself, such as system-level logs.</li>
 	<li>The hostPath volume type is one of the most dangerous volume types in Kubernetes and is usually reserved for use in privileged pods only.</li>
 	<li><strong>Note:</strong> When the type is FileOrCreate or DirectoryOrCreate and Kubernetes needs to create the file/directory, its file permissions are set to 644 (rw-r--r--) and 755 (rwxr-xr-x), respectively. In either case, the file/directory is owned by the user and group used to run the Kubelet.</li>
</ul>
<h2>Chapter 8: Configuring applications using ConfigMaps, Secrets, and the Downward
API</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 9: Organizing API objects using labels, selectors, and Namespaces</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 10: Exposing Pods with Services and Ingresses</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 11: Deploying applications using Deployments</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 12: Persisting application data with PersistentVolumes</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 13: Deploying distributed applications using StatefulSets</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 14: Running special workloads using DaemonSets, Jobs, and CronJobs</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 15: Understanding the fine details of the Kubernetes API</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 16: Diving deep into the Control Plane</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter  17: Diving deep into the Worker Nodes</h2>
<ul>
 	<li></li>
</ul>
<h2>Chapter 18: Understanding the internal operation of Kubernetes controllers</h2>
<h2>Chapter 19: Deploying highly-available clusters</h2>
<h2>Chapter 20 Managing the computing resources available to Pods</h2>
<h2>Chapter 21 Advanced scheduling using affinity and anti-affinity</h2>
<h2>Chapter 22 Automatic scaling using the HorizontalPodAutoscaler</h2>
<h2>Chapter 23 Securing the Kubernetes API using RBAC</h2>
<h2>Chapter 24 Protecting cluster nodes with PodSecurityPolicies</h2>
<h2>Chapter 25 Locking down network communication using NetworkPolicies</h2>
<h2>Chapter 26 Upgrading, backing up, and restoring Kubernetes clusters</h2>
<h2>Chapter 27 Adding centralized logging, metrics, alerting, and tracing</h2>
<h2>Chapter 28: Best practices for Kubernetes application development and deployment</h2>
<h2>Chapter 29: Extending Kubernetes with CustomResourceDefinitions and operators</h2>
&nbsp;
